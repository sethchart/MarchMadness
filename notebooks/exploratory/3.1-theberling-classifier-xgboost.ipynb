{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we do with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# Import RandomForestClassifier and GradientBoostingClassifer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Function for splitting training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Function for creating model pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# For standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Helper for cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the dataset from the processed data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/processed/model_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>LowID</th>\n",
       "      <th>HighID</th>\n",
       "      <th>Win</th>\n",
       "      <th>LowScore</th>\n",
       "      <th>LowFGP</th>\n",
       "      <th>LowFGP3</th>\n",
       "      <th>LowFTP</th>\n",
       "      <th>LowOR</th>\n",
       "      <th>LowDR</th>\n",
       "      <th>...</th>\n",
       "      <th>HighFTP</th>\n",
       "      <th>HighOR</th>\n",
       "      <th>HighDR</th>\n",
       "      <th>HighAst</th>\n",
       "      <th>HighTO</th>\n",
       "      <th>HighStl</th>\n",
       "      <th>HighBlk</th>\n",
       "      <th>HighPF</th>\n",
       "      <th>HighRank</th>\n",
       "      <th>HighSeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>1411</td>\n",
       "      <td>1421</td>\n",
       "      <td>0</td>\n",
       "      <td>72.800000</td>\n",
       "      <td>0.448892</td>\n",
       "      <td>0.321414</td>\n",
       "      <td>0.613745</td>\n",
       "      <td>13.166667</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766142</td>\n",
       "      <td>12.275862</td>\n",
       "      <td>23.172414</td>\n",
       "      <td>13.034483</td>\n",
       "      <td>16.206897</td>\n",
       "      <td>7.068966</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>19.103448</td>\n",
       "      <td>240.343750</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003</td>\n",
       "      <td>1112</td>\n",
       "      <td>1436</td>\n",
       "      <td>1</td>\n",
       "      <td>85.214286</td>\n",
       "      <td>0.463563</td>\n",
       "      <td>0.351060</td>\n",
       "      <td>0.701154</td>\n",
       "      <td>15.178571</td>\n",
       "      <td>27.642857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649708</td>\n",
       "      <td>12.965517</td>\n",
       "      <td>25.724138</td>\n",
       "      <td>14.206897</td>\n",
       "      <td>14.068966</td>\n",
       "      <td>6.862069</td>\n",
       "      <td>2.965517</td>\n",
       "      <td>15.896552</td>\n",
       "      <td>153.125000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>1113</td>\n",
       "      <td>1272</td>\n",
       "      <td>1</td>\n",
       "      <td>75.965517</td>\n",
       "      <td>0.481680</td>\n",
       "      <td>0.328376</td>\n",
       "      <td>0.675667</td>\n",
       "      <td>13.689655</td>\n",
       "      <td>23.310345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628299</td>\n",
       "      <td>14.068966</td>\n",
       "      <td>25.965517</td>\n",
       "      <td>16.620690</td>\n",
       "      <td>13.793103</td>\n",
       "      <td>7.379310</td>\n",
       "      <td>5.068966</td>\n",
       "      <td>18.758621</td>\n",
       "      <td>21.705882</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>1141</td>\n",
       "      <td>1166</td>\n",
       "      <td>1</td>\n",
       "      <td>79.344828</td>\n",
       "      <td>0.506349</td>\n",
       "      <td>0.377481</td>\n",
       "      <td>0.762741</td>\n",
       "      <td>10.586207</td>\n",
       "      <td>23.275862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689707</td>\n",
       "      <td>10.878788</td>\n",
       "      <td>23.181818</td>\n",
       "      <td>16.818182</td>\n",
       "      <td>13.363636</td>\n",
       "      <td>8.393939</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>17.272727</td>\n",
       "      <td>20.735294</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003</td>\n",
       "      <td>1143</td>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "      <td>74.482759</td>\n",
       "      <td>0.468741</td>\n",
       "      <td>0.375934</td>\n",
       "      <td>0.688632</td>\n",
       "      <td>11.241379</td>\n",
       "      <td>24.379310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778148</td>\n",
       "      <td>9.733333</td>\n",
       "      <td>22.033333</td>\n",
       "      <td>14.666667</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>7.766667</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>50.312500</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  LowID  HighID  Win   LowScore    LowFGP   LowFGP3    LowFTP  \\\n",
       "0    2003   1411    1421    0  72.800000  0.448892  0.321414  0.613745   \n",
       "1    2003   1112    1436    1  85.214286  0.463563  0.351060  0.701154   \n",
       "2    2003   1113    1272    1  75.965517  0.481680  0.328376  0.675667   \n",
       "3    2003   1141    1166    1  79.344828  0.506349  0.377481  0.762741   \n",
       "4    2003   1143    1301    1  74.482759  0.468741  0.375934  0.688632   \n",
       "\n",
       "       LowOR      LowDR  ...   HighFTP     HighOR     HighDR    HighAst  \\\n",
       "0  13.166667  24.800000  ...  0.766142  12.275862  23.172414  13.034483   \n",
       "1  15.178571  27.642857  ...  0.649708  12.965517  25.724138  14.206897   \n",
       "2  13.689655  23.310345  ...  0.628299  14.068966  25.965517  16.620690   \n",
       "3  10.586207  23.275862  ...  0.689707  10.878788  23.181818  16.818182   \n",
       "4  11.241379  24.379310  ...  0.778148   9.733333  22.033333  14.666667   \n",
       "\n",
       "      HighTO   HighStl   HighBlk     HighPF    HighRank  HighSeed  \n",
       "0  16.206897  7.068966  3.000000  19.103448  240.343750      16.0  \n",
       "1  14.068966  6.862069  2.965517  15.896552  153.125000      16.0  \n",
       "2  13.793103  7.379310  5.068966  18.758621   21.705882       7.0  \n",
       "3  13.363636  8.393939  4.454545  17.272727   20.735294       6.0  \n",
       "4  14.200000  7.766667  3.066667  18.666667   50.312500       9.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['Unnamed: 0'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split the data so it's the target and input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate object for target variable\n",
    "y = df.Win\n",
    "\n",
    "# Create separate object for input features\n",
    "X = df.drop('Win', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and test data with 20% of the observations going to the test set. We also give it a random state so we can reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892 223 892 223\n"
     ]
    }
   ],
   "source": [
    "# Split X and y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1234)\n",
    "\n",
    "# Print number of observations in X_train, X_test, y_train, and y_test\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok time for pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline dictionary\n",
    "pipelines = {\n",
    "    'xg': make_pipeline(StandardScaler(),XGBClassifier(random_state=123)),\n",
    "    'gb': make_pipeline(StandardScaler(),GradientBoostingClassifier(random_state=123))\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted Tree hyperparameters\n",
    "gb_hyperparameters = {'gradientboostingclassifier__n_estimators': [200],\n",
    "                     'gradientboostingclassifier__learning_rate': [0.05],\n",
    "                     'gradientboostingclassifier__max_depth': [1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted Tree hyperparameters\n",
    "xg_hyperparameters = {\n",
    "'xgbclassifier__max_depth': range (2, 10, 1),\n",
    "    'xgbclassifier__n_estimators': range(60, 220, 40),\n",
    "    'xgbcalssifier__learning_rate': [0.1, 0.01, 0.05]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hyperparameters dictionary\n",
    "hyperparameters = {\n",
    "    'xg': xg_hyperparameters,\n",
    "    'gb': gb_hyperparameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter xgbcalssifier for estimator Pipeline(memory=None,\n         steps=[('standardscaler',\n                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n                ('xgbclassifier',\n                 XGBClassifier(base_score=0.5, booster='gbtree',\n                               colsample_bylevel=1, colsample_bynode=1,\n                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n                               max_delta_step=0, max_depth=3,\n                               min_child_weight=1, missing=nan,\n                               n_estimators=100, n_jobs=1, nthread=None,\n                               objective='binary:logistic', random_state=123,\n                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n                               seed=None, silent=None, subsample=1,\n                               verbosity=1))],\n         verbose=False). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 608, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 256, in __call__\n    for func, args, kwargs in self.items]\n  File \"/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 256, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 504, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 163, in set_params\n    self._set_params('steps', **kwargs)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\", line 50, in _set_params\n    super().set_params(**params)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py\", line 236, in set_params\n    (key, self))\nValueError: Invalid parameter xgbcalssifier for estimator Pipeline(memory=None,\n         steps=[('standardscaler',\n                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n                ('xgbclassifier',\n                 XGBClassifier(base_score=0.5, booster='gbtree',\n                               colsample_bylevel=1, colsample_bynode=1,\n                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n                               max_delta_step=0, max_depth=3,\n                               min_child_weight=1, missing=nan,\n                               n_estimators=100, n_jobs=1, nthread=None,\n                               objective='binary:logistic', random_state=123,\n                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n                               seed=None, silent=None, subsample=1,\n                               verbosity=1))],\n         verbose=False). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-8da71c914e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Fit model on X_train, y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Store model in fitted_models[name]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter xgbcalssifier for estimator Pipeline(memory=None,\n         steps=[('standardscaler',\n                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n                ('xgbclassifier',\n                 XGBClassifier(base_score=0.5, booster='gbtree',\n                               colsample_bylevel=1, colsample_bynode=1,\n                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n                               max_delta_step=0, max_depth=3,\n                               min_child_weight=1, missing=nan,\n                               n_estimators=100, n_jobs=1, nthread=None,\n                               objective='binary:logistic', random_state=123,\n                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n                               seed=None, silent=None, subsample=1,\n                               verbosity=1))],\n         verbose=False). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "# Create empty dictionary called fitted_models\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "for name, pipeline in pipelines.items():\n",
    "    # Create cross-validation object from pipeline and hyperparameters\n",
    "    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1, scoring='neg_log_loss')\n",
    "    \n",
    "    # Fit model on X_train, y_train\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    # Store model in fitted_models[name] \n",
    "    fitted_models[name] = model\n",
    "    \n",
    "    # Print '{name} has been fitted'\n",
    "    print(name, 'has beed fitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cannot figure out how to get grid search to work on the XGBClassifier so I'm just going to play around by hand until I get decent accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
    "       gamma=0, learning_rate=0.5, max_delta_step=2, max_depth=10,\n",
    "       min_child_weight=1, missing=None, n_estimators=300, nthread=-1,reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.5, max_delta_step=2, max_depth=10,\n",
       "              min_child_weight=1, missing=None, n_estimators=300, n_jobs=1,\n",
       "              nthread=-1, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0,\n",
       "              silent=True, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7130044843049327\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to be pretty decent. Most of the values that I was playing with gave me somewhere between 66 and 70. Let's check it against the winning gradient boosting model from 3.0-theberling-classifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GradientBoostingClassifier(n_estimators=200,learning_rate=0.05,max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.05, loss='deviance', max_depth=1,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model2.predict(X_test)\n",
    "predictions2 = [round(value) for value in y_pred2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6860986547085202\n"
     ]
    }
   ],
   "source": [
    "accuracy2 = accuracy_score(y_test, predictions2)\n",
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it on our submission data set and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>LowID</th>\n",
       "      <th>HighID</th>\n",
       "      <th>LowScore</th>\n",
       "      <th>LowFGP</th>\n",
       "      <th>LowFGP3</th>\n",
       "      <th>LowFTP</th>\n",
       "      <th>LowOR</th>\n",
       "      <th>LowDR</th>\n",
       "      <th>LowAst</th>\n",
       "      <th>...</th>\n",
       "      <th>HighFTP</th>\n",
       "      <th>HighOR</th>\n",
       "      <th>HighDR</th>\n",
       "      <th>HighAst</th>\n",
       "      <th>HighTO</th>\n",
       "      <th>HighStl</th>\n",
       "      <th>HighBlk</th>\n",
       "      <th>HighPF</th>\n",
       "      <th>HighRank</th>\n",
       "      <th>HighSeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>1107</td>\n",
       "      <td>1112</td>\n",
       "      <td>65.5</td>\n",
       "      <td>0.44403</td>\n",
       "      <td>0.358773</td>\n",
       "      <td>0.728865</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.625</td>\n",
       "      <td>10.4375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699585</td>\n",
       "      <td>10.823529</td>\n",
       "      <td>26.411765</td>\n",
       "      <td>14.205882</td>\n",
       "      <td>11.205882</td>\n",
       "      <td>7.176471</td>\n",
       "      <td>3.588235</td>\n",
       "      <td>17.911765</td>\n",
       "      <td>4.209677</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>1107</td>\n",
       "      <td>1116</td>\n",
       "      <td>65.5</td>\n",
       "      <td>0.44403</td>\n",
       "      <td>0.358773</td>\n",
       "      <td>0.728865</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.625</td>\n",
       "      <td>10.4375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717672</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>22.764706</td>\n",
       "      <td>16.147059</td>\n",
       "      <td>11.735294</td>\n",
       "      <td>7.764706</td>\n",
       "      <td>4.764706</td>\n",
       "      <td>18.970588</td>\n",
       "      <td>22.306452</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>1107</td>\n",
       "      <td>1124</td>\n",
       "      <td>65.5</td>\n",
       "      <td>0.44403</td>\n",
       "      <td>0.358773</td>\n",
       "      <td>0.728865</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.625</td>\n",
       "      <td>10.4375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668958</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>24.031250</td>\n",
       "      <td>14.406250</td>\n",
       "      <td>12.375000</td>\n",
       "      <td>8.031250</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>16.718750</td>\n",
       "      <td>14.177419</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>1107</td>\n",
       "      <td>1125</td>\n",
       "      <td>65.5</td>\n",
       "      <td>0.44403</td>\n",
       "      <td>0.358773</td>\n",
       "      <td>0.728865</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.625</td>\n",
       "      <td>10.4375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695699</td>\n",
       "      <td>9.322581</td>\n",
       "      <td>23.967742</td>\n",
       "      <td>15.193548</td>\n",
       "      <td>13.741935</td>\n",
       "      <td>6.354839</td>\n",
       "      <td>1.967742</td>\n",
       "      <td>16.451613</td>\n",
       "      <td>129.724138</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>1107</td>\n",
       "      <td>1129</td>\n",
       "      <td>65.5</td>\n",
       "      <td>0.44403</td>\n",
       "      <td>0.358773</td>\n",
       "      <td>0.728865</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.625</td>\n",
       "      <td>10.4375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729914</td>\n",
       "      <td>8.516129</td>\n",
       "      <td>24.032258</td>\n",
       "      <td>11.709677</td>\n",
       "      <td>10.225806</td>\n",
       "      <td>5.967742</td>\n",
       "      <td>2.322581</td>\n",
       "      <td>17.225806</td>\n",
       "      <td>46.700000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  LowID  HighID  LowScore   LowFGP   LowFGP3    LowFTP  LowOR  \\\n",
       "0    2015   1107    1112      65.5  0.44403  0.358773  0.728865   10.0   \n",
       "1    2015   1107    1116      65.5  0.44403  0.358773  0.728865   10.0   \n",
       "2    2015   1107    1124      65.5  0.44403  0.358773  0.728865   10.0   \n",
       "3    2015   1107    1125      65.5  0.44403  0.358773  0.728865   10.0   \n",
       "4    2015   1107    1129      65.5  0.44403  0.358773  0.728865   10.0   \n",
       "\n",
       "    LowDR   LowAst  ...   HighFTP     HighOR     HighDR    HighAst     HighTO  \\\n",
       "0  23.625  10.4375  ...  0.699585  10.823529  26.411765  14.205882  11.205882   \n",
       "1  23.625  10.4375  ...  0.717672  13.000000  22.764706  16.147059  11.735294   \n",
       "2  23.625  10.4375  ...  0.668958  14.500000  24.031250  14.406250  12.375000   \n",
       "3  23.625  10.4375  ...  0.695699   9.322581  23.967742  15.193548  13.741935   \n",
       "4  23.625  10.4375  ...  0.729914   8.516129  24.032258  11.709677  10.225806   \n",
       "\n",
       "    HighStl   HighBlk     HighPF    HighRank  HighSeed  \n",
       "0  7.176471  3.588235  17.911765    4.209677       2.0  \n",
       "1  7.764706  4.764706  18.970588   22.306452       5.0  \n",
       "2  8.031250  3.843750  16.718750   14.177419       3.0  \n",
       "3  6.354839  1.967742  16.451613  129.724138      15.0  \n",
       "4  5.967742  2.322581  17.225806   46.700000      11.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/processed/model_dataset2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9958932e-01, 4.1067391e-04],\n",
       "       [9.9694955e-01, 3.0504181e-03],\n",
       "       [9.8525810e-01, 1.4741913e-02],\n",
       "       ...,\n",
       "       [3.7697077e-01, 6.2302923e-01],\n",
       "       [7.4967623e-02, 9.2503238e-01],\n",
       "       [2.4914742e-03, 9.9750853e-01]], dtype=float32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict_proba(data)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.1067391e-04, 3.0504181e-03, 1.4741913e-02, ..., 6.2302923e-01,\n",
       "       9.2503238e-01, 9.9750853e-01], dtype=float32)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_of_1 = probs[:,1]\n",
    "prob_of_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_id(row):\n",
    "    season = row[0]\n",
    "    low_id = row[1]\n",
    "    high_id = row[2]\n",
    "    ID = str(int(season))+'_'+str(int(low_id))+'_'+str(int(high_id))\n",
    "    return ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = data.apply(make_id,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015_1107_1112</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015_1107_1116</td>\n",
       "      <td>0.003050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015_1107_1124</td>\n",
       "      <td>0.014742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015_1107_1125</td>\n",
       "      <td>0.382413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015_1107_1129</td>\n",
       "      <td>0.019238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID      Pred\n",
       "0  2015_1107_1112  0.000411\n",
       "1  2015_1107_1116  0.003050\n",
       "2  2015_1107_1124  0.014742\n",
       "3  2015_1107_1125  0.382413\n",
       "4  2015_1107_1129  0.019238"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission['ID'] = ID\n",
    "df_submission['Pred'] = prob_of_1\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('../../data/predictions/phase1_submissions2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
